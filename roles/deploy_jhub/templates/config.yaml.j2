# Copy this file to config.yaml and fill everything <TO_FILL>
proxy:
  https:
    enabled: true
    hosts:
      - "{{ host_domain }}"
    letsencrypt:
{% if staging_cert is true %}
      acmeServer: "https://acme-staging-v02.api.letsencrypt.org/directory"
{% else %}
      acmeServer: "https://acme-v02.api.letsencrypt.org/directory"
{% endif %}
      # This email gets warnings if auto-renewal fails
      contactEmail: "{{ contact_email }}"
  # This can be generated with `openssl rand -hex 32`
  service:
    loadBalancerIP: "{{ ip_address }}"

singleuser:
  # Use new UI by default as the old UI is being deprecated
  #  defaultUrl: "/lab"
  defaultUrl: "/lab"
  extraEnv:
    JUPYTERHUB_ACTIVITY_INTERVAL: "3600"
  events: true

  storage:
    # Required for PyTorch
    extraVolumes:
      - name: shm-volume
        emptyDir:
          medium: Memory
      - name: training-materials
        persistentVolumeClaim:
          claimName: training-materials
    extraVolumeMounts:
      - name: shm-volume
        mountPath: /dev/shm
      - name: training-materials
        mountPath: /mnt/materials

    type: dynamic
    capacity: 10Gi
    dynamic:
      storageClass: longhorn

  # Default profile, this is what the placeholders will use
  startTimeout: 3600 # seconds == 60 minutes
  cpu:
    limit: 2
    guarantee: 0.05
  memory:
    limit: 1.8G
    guarantee: 1.8G

  profileList:
  {% for profile in profiles %}
   - display_name: "{{ profile.display_name }}"
       description: |
         "{{ profile.description }}"
       image: consideratio/singleuser-gpu:v0.3.0
       default: {{ profile.default }}
       kubespawner_override:
         image: {{ profile.image }}
         cpu_limit: {{ profile.cpu_limit }}
         cpu_guarantee: {{ profile.cpu_guarantee }}
         mem_limit: "{{ profile.mem_limit }}"
         mem_guarantee: "{{ profile.mem_guarantee }}"
{% if profile.use_gpus is defined %}
         tolerations:
           - key: {{ profile.GPU_info.key }}
             operator: {{ profile.GPU_info.operator }}
             effect: {{ profile.GPU_info.effect }}
         extra_resource_limits:
            nvidia.com/gpu: "{{ profile.GPU_info.number_of_gpus}}"
{% endif %}

  {% if (profile.commands | default([]) | select('truthy') | list) | length > 0 %}
       lifecycle_hooks:
           postStart:
             exec:
               command:
                 - "bash"
                 - "-c"
                 - |
  {% for command in profile.commands %}
                 {{ command }} || true
  {% endfor %}
  {% endif %}

  {% endfor %}
  
hub:
  config:
    Authenticator:
    {% if iris_iam is defined %}
    auto_login: false
    GenericOAuthenticator:
      client_id: {{ client_id }}
      client_secret: {{ client_secret }}
      oauth_callback_url: "https://{{ host_domain }}/hub/oauth_callback"
      authorize_url: https://iris-iam.stfc.ac.uk/authorize
      token_url: https://iris-iam.stfc.ac.uk/token
      userdata_url: https://iris-iam.stfc.ac.uk/userinfo
      scope:
        - openid
        - preferred_username
        - profile
        - email
      username_claim: preferred_username
      userdata_params:
        state: state
      claim_groups_key: groups
      allowed_groups:
{% for allowed_group in allowed_groups %}
        - {{ allowed_group }}
{% endfor %}
      admin_groups:
{% for admin_group in admin_groups %}
        - {{ admin_group }}
{% endfor %}
      manage_groups: true
    JupyterHub:
      authenticator_class: generic-oauth

     {% else %}
      admin_users:
  #  Weird indentation is expected in .j2 file
  {% for admin_name in (admin_names) %}
    - admin-{{ admin_name }}
  {% endfor %}
    check_common_password: true
      open_signup: false
      allowed_users:
  #  Weird indentation is expected in .j2 file
{% set total_n_users = number_of_users | int %}
{% set width_n_users = ("%d" % total_n_users).__len__() %}
{% for i in range (1, (number_of_users | int) + 1) %}
      - {{ username }}-{{ "%0*d" % (width_n_users, i) }}
{% endfor %}

    JupyterHub:
      authenticator_class: nativeauthenticator.NativeAuthenticator
    {% endif %}

  consecutiveFailureLimit: 0
  extraConfig:
    myConfig: |
      c.JupyterHub.activity_resolution = 6000
      c.JupyterHub.last_activity_interval = 300
      c.JupyterHub.init_spawners_timeout = 1
      c.JupyterHub.concurrent_spawn_limit = 2000
      c.KubeSpawner.k8s_api_threadpool_workers = c.JupyterHub.concurrent_spawn_limit
      c.KubeSpawner.http_timeout = 60

scheduling:
  # Try to pack users tightly rather than spinning up one node per user
  userScheduler:
    enabled: false
  # Enable us to evict a placeholder rather than spin up a whole new node
  podPriority:
    enabled: true
    defaultPriority: 0
    userPlaceholderPriority: -10 # -10 is equal to scale up criteria
  # Prep some environments beforehand so users don't have to wait
  userPlaceholder:
    enabled: true
    replicas: 4 # Number of placeholders, this should eq high mem / default mem

cull:
  enabled: true
  timeout: 432000 # seconds == 5 day
  every: 3600 # Run once an hour instead of every 10 minutes
  concurrency: 1

