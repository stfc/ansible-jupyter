# Copy this file to config.yaml and fill everything <TO_FILL>
proxy:
  https:
    enabled: true
    hosts:
      - "{{ host_domain }}"
    letsencrypt:
{% if staging_cert is true %}
      acmeServer: "https://acme-staging-v02.api.letsencrypt.org/directory"
{% endif %}
      # This email gets warnings if auto-renewal fails
      contactEmail: "{{ contact_email }}"
  # This can be generated with `openssl rand -hex 32`
  service:
    loadBalancerIP: "{{ ip_address }}"

singleuser:
  # Use new UI by default as the old UI is being deprecated
  #  defaultUrl: "/lab"
  defaultUrl: "/lab"
  extraEnv:
    JUPYTERHUB_ACTIVITY_INTERVAL: "3600"
  events: true

  storage:
    # Required for PyTorch
    extraVolumes:
      - name: shm-volume
        emptyDir:
          medium: Memory
      - name: training-materials
        persistentVolumeClaim:
          claimName: training-materials
    extraVolumeMounts:
      - name: shm-volume
        mountPath: /dev/shm
      - name: training-materials
        mountPath: /mnt/materials

    type: dynamic
    capacity: 10Gi
    dynamic:
      storageClass: longhorn

  # Default profile, this is what the placeholders will use
  startTimeout: 3600 # seconds == 60 minutes
  cpu:
    limit: 2
    guarantee: 0.05
  memory:
    limit: 1.8G
    guarantee: 1.8G

  profileList:
  {% for profile in profiles %}
    - display_name: "{{ profile.display_name }}"
        description: |
          "{{ profile.description }}"
        default: {{ profile.default }}
        kubespawner_override:
          image: "{{ profile.image }}"

  {% if profile.use_gpus is defined %}
        tolerations:
          - key: {{ profile.GPU_info.key }}
            operator: {{ profile.GPU_info.operator }}
            effect: {{ profile.GPU_info.effect }}
        extra_resource_limits:
          nvidia.com/gpu: {{ profile.GPU_info.number_of_gpus}}
  {% endif %}

  {% if profile.commands is defined %}
        lifecycle_hooks:
            postStart:
              exec:
                command:
                  - "bash"
                  - "-c"
                  - |
  {% for command in profile.commands %}
                  {{ command }} || true
  {% endfor %}
  {% endif %}

  {% endfor %}

hub:
  config:  
    Authenticator:
    {% if iris_iam is true %}
      auto_login: false
  
    GenericOAuthenticator:
      login_service: "IRIS IAM" # Name for the shiny button
      client_id: {{ client_id }}
      client_secret: {{ client_secret }}
      # registration_token: {{ registration_token }}
      oauth_callback_url: "https://{{ JUPYTER_DOMAIN_NAME }}/hub/oauth_callback"
      authorize_url: https://iris-iam.stfc.ac.uk/authorize
      token_url: https://iris-iam.stfc.ac.uk/token
      userdata_url: https://iris-iam.stfc.ac.uk/userinfo
      scope:
        - openid
        - preferred_username
        - profile
        - email
      username_claim: preferred_username
      userdata_params:
        state: state
      claim_groups_key: groups
      allowed_groups:
        - stfc-cloud
      admin_groups:
        - stfc-cloud
    JupyterHub:
      authenticator_class: generic-oauth
    {% endif %}

      admin_users:
  #  Weird indentation is expected in .j2 file
  {% for admin_name in (admin_names) %}
    - admin-{{ admin_name }}
  {% endfor %}
    check_common_password: true
      open_signup: false
      {% if iris_iam is false %}
      allowed_users:
  #  Weird indentation is expected in .j2 file
{% set total_n_users = number_of_users | int %}
{% set width_n_users = ("%d" % total_n_users).__len__() %}
{% for i in range (1, (number_of_users | int) + 1) %}
      - {{ username }}-{{ "%0*d" % (width_n_users, i) }}
{% endfor %}
      {% endif %}
    
    JupyterHub:
      authenticator_class: nativeauthenticator.NativeAuthenticator
  consecutiveFailureLimit: 0
  extraConfig:
    myConfig: |
      c.JupyterHub.activity_resolution = 6000
      c.JupyterHub.last_activity_interval = 300
      c.JupyterHub.init_spawners_timeout = 1
      c.JupyterHub.concurrent_spawn_limit = 2000
      c.KubeSpawner.k8s_api_threadpool_workers = c.JupyterHub.concurrent_spawn_limit
      c.KubeSpawner.http_timeout = 60

scheduling:
  # Try to pack users tightly rather than spinning up one node per user
  userScheduler:
    enabled: false
  # Enable us to evict a placeholder rather than spin up a whole new node
  podPriority:
    enabled: true
    defaultPriority: 0
    userPlaceholderPriority: -10 # -10 is equal to scale up criteria
  # Prep some environments beforehand so users don't have to wait
  userPlaceholder:
    enabled: true
    replicas: 4 # Number of placeholders, this should eq high mem / default mem

cull:
  enabled: true
  timeout: 432000 # seconds == 5 day
  every: 3600 # Run once an hour instead of every 10 minutes
  concurrency: 1

