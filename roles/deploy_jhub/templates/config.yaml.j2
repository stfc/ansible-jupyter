# Copy this file to config.yaml and fill everything <TO_FILL>
proxy:
  https:
    enabled: true
    hosts:
      - "{{ host_domain }}"
    letsencrypt:
{% if staging_cert is true %}
      acmeServer: "https://acme-staging-v02.api.letsencrypt.org/directory"
{% else %}
      acmeServer: "https://acme-v02.api.letsencrypt.org/directory"
{% endif %}
      # This email gets warnings if auto-renewal fails
      contactEmail: "{{ contact_email }}"
  # This can be generated with `openssl rand -hex 32`
  service:
    loadBalancerIP: "{{ ip_address }}"

singleuser:
  # Use new UI by default as the old UI is being deprecated
  #  defaultUrl: "/lab"
  defaultUrl: "/lab"
  extraEnv:
    JUPYTERHUB_ACTIVITY_INTERVAL: "3600"
  events: true

  storage:
    # Required for PyTorch
    extraVolumes:
      - name: shm-volume
        emptyDir:
          medium: Memory
      - name: training-materials
        persistentVolumeClaim:
          claimName: training-materials
    extraVolumeMounts:
      - name: shm-volume
        mountPath: /dev/shm
      - name: training-materials
        mountPath: /mnt/materials

    type: dynamic
    capacity: 10Gi
    dynamic:
      storageClass: longhorn

  # Default profile, this is what the placeholders will use
  startTimeout: 3600 # seconds == 60 minutes
  cpu:
    limit: 2
    guarantee: 0.05
  memory:
    limit: 1.8G
    guarantee: 1.8G

  profileList:
  {% for profile in profiles %}
    - display_name: "{{ profile.display_name }}"
        description: |
          "{{ profile.description }}"
        default: {{ profile.default }}
        kubespawner_override:
          image: "{{ profile.image }}"

  {% if profile.use_gpus is defined %}
        tolerations:
          - key: {{ profile.GPU_info.key }}
            operator: {{ profile.GPU_info.operator }}
            effect: {{ profile.GPU_info.effect }}
        extra_resource_limits:
          nvidia.com/gpu: {{ profile.GPU_info.number_of_gpus}}
  {% endif %}

  {% if profile.commands is defined %}
        lifecycle_hooks:
            postStart:
              exec:
                command:
                  - "bash"
                  - "-c"
                  - |
  {% for command in profile.commands %}
                  {{ command }} || true
  {% endfor %}
  {% endif %}

  {% endfor %}

hub:
  config:
    Authenticator:
    {% if iris_iam is defined %}
    auto_login: false
    GenericOAuthenticator:
      client_id: {{ client_id }}
      client_secret: {{ client_secret }}
      oauth_callback_url: "https://{{ host_domain }}/hub/oauth_callback"
      authorize_url: https://iris-iam.stfc.ac.uk/authorize
      token_url: https://iris-iam.stfc.ac.uk/token
      userdata_url: https://iris-iam.stfc.ac.uk/userinfo
      scope:
        - openid
        - preferred_username
        - profile
        - email
      username_claim: preferred_username
      userdata_params:
        state: state
      claim_groups_key: groups
      allowed_groups:
{% for allowed_group in allowed_groups %}
        - {{ allowed_group }}
{% endfor %}
      admin_groups:
{% for admin_group in admin_groups %}
        - {{ admin_group }}
{% endfor %}
      manage_groups: true
    JupyterHub:
      authenticator_class: generic-oauth

     {% else %}
      admin_users:
  #  Weird indentation is expected in .j2 file
  {% for admin_name in (admin_names) %}
    - admin-{{ admin_name }}
  {% endfor %}
    check_common_password: true
      open_signup: false
      allowed_users:
  #  Weird indentation is expected in .j2 file
{% for prefix, count in user_groups.items() %}
{%   set width_n_users = ("%d" % (count | int)).__len__() %}
{%   for i in range(1, (count | int) + 1) %}
      - {{ prefix }}-{{ "%0*d" % (width_n_users, i) }}
{%   endfor %}
{% endfor %}

    JupyterHub:
      authenticator_class: nativeauthenticator.NativeAuthenticator
    {% endif %}

  consecutiveFailureLimit: 0
  extraConfig:
    myConfig: |
      c.JupyterHub.activity_resolution = 6000
      c.JupyterHub.last_activity_interval = 300
      c.JupyterHub.init_spawners_timeout = 1
      c.JupyterHub.concurrent_spawn_limit = 2000
      c.KubeSpawner.k8s_api_threadpool_workers = c.JupyterHub.concurrent_spawn_limit
      c.KubeSpawner.http_timeout = 60

scheduling:
  # Try to pack users tightly rather than spinning up one node per user
  userScheduler:
    enabled: false
  # Enable us to evict a placeholder rather than spin up a whole new node
  podPriority:
    enabled: true
    defaultPriority: 0
    userPlaceholderPriority: -10 # -10 is equal to scale up criteria
  # Prep some environments beforehand so users don't have to wait
  userPlaceholder:
    enabled: true
    replicas: 4 # Number of placeholders, this should eq high mem / default mem

cull:
  enabled: true
  timeout: 432000 # seconds == 5 day
  every: 3600 # Run once an hour instead of every 10 minutes
  concurrency: 1

