# Copy this file to config.yaml and fill everything <TO_FILL>
proxy:
  https:
    enabled: true
    hosts:
      - jupyter.stfc.ac.uk
    letsencrypt:
      # Uncomment acmeserver for a test certificate
      # acmeServer: "https://acme-staging-v02.api.letsencrypt.org/directory"
      # This email gets warnings if auto-renewal fails
      contactEmail: <TO_FILL>
  # This can be generated with `openssl rand -hex 32`
  secretToken: <TO_FILL>

singleuser:
  # Use new UI by default as the old UI is being deprecated
  defaultUrl: "/lab"

  storage:
    # Required for PyTorch
    extraVolumes:
      - name: shm-volume
        emptyDir:
          medium: Memory
    extraVolumeMounts:
      - name: shm-volume
        mountPath: /dev/shm

    type: dynamic
    capacity: 10Gi # 1GB is the minimum Cinder will serve
    dynamic:
      storageClass: cinder-storage

  # Default profile, this is what the placeholders will use
  startTimeout: 1200 # seconds == 20 minutes
  cpu:
    limit: 2
    guarantee: 0.05
  memory:
    limit: 1.8G
    guarantee: 1.8G

  profileList:
    - display_name: "Default: Normal Size"
      description: |
        For small jobs and prototyping: 2 CPUs, 1.5GB RAM and no GPU. This is the default, and will usually start in ~2 minutes. During periods of high-contention it may take up to 20 minutes to create.
      default: True
    - display_name: "Large Memory Instance"
      description: |
        For larger jobs. 8 CPUs, 7.5GB RAM and no GPU. This may
        take up to 20 minutes to create.
      kubespawner_override:
        cpu_limit: 8
        cpu_guarantee: 0.05
        mem_limit: "7.5G"
        mem_guarantee: "7.5G"
        extra_resource_limits: {}
    # - display_name: "GPU"
    #   description: |
    #     This configuration gives you 2 CPUs, 3GB of RAM, and a GPU
    #   image: consideratio/singleuser-gpu:v0.3.0
    #   kubespawner_override:
    #     cpu_limit: 2
    #     cpu_guarantee: 0.05
    #     mem_limit: "3G"
    #     mem_guarantee: "3G"
    #     tolerations:
    #       - key: nvidia.com/gpu
    #         operator: Equal
    #         value: "present"
    #         effect: NoSchedule
    #     extra_resource_limits:
    #       nvidia.com/gpu: "1"

hub:
  config:
    Authenticator:
      # Whilst auto-login would be nice it dumps users at IRIS IAM with no warning
      # this gets users into the habit of blindly accepting sign-in without seeing the service
      auto_login: false
    GenericOAuthenticator:
      login_service: "IRIS IAM" # Name for the shiny button
      client_id: <TO_FILL>
      client_secret: <TO_FILL>
      # registration_token: <TO_FILL>
      oauth_callback_url: https://jupyter.stfc.ac.uk/hub/oauth_callback
      authorize_url: https://iris-iam.stfc.ac.uk/authorize
      token_url: https://iris-iam.stfc.ac.uk/token
      userdata_url: https://iris-iam.stfc.ac.uk/userinfo
      scope:
        - openid
        - preferred_username
        - profile
        - email
        - groups
      username_key: preferred_username
    JupyterHub:
      # See below why this is commented out
      # authenticator_class: GroupAuth
  extraConfig:
    custom_auth: |
      from typing import List
      from oauthenticator.generic import GenericOAuthenticator

      class GroupAuth(GenericOAuthenticator):
          allowed_groups: List[str] = [
              "stfc-cloud"
          ]

          admin_groups: List[str] = [
              "stfc-cloud"
          ]

          # Adapted from https://github.com/jupyterhub/oauthenticator/pull/395/files
          # When it's merged this can and should be removed

          claim_groups_key = "groups"
          @staticmethod
          def check_user_in_groups(member_groups, allowed_groups):
              return bool(set(member_groups) & set(allowed_groups))

          async def authenticate(self, *args, **kwargs):
              user_info = await super().authenticate(*args, **kwargs)
              if self.allowed_groups:
                  self.log.info('Validating if user claim groups match any of {}'.format(self.allowed_groups))
                  user_data_resp_json = user_info["auth_state"]["oauth_user"]

                  if callable(self.claim_groups_key):
                      groups = self.claim_groups_key(user_data_resp_json)
                  else:
                      groups = user_data_resp_json.get(self.claim_groups_key)

                  if not groups:
                      self.log.error(
                          "No claim groups found for user! Something wrong with the `claim_groups_key` {}? {}".format(
                              self.claim_groups_key, user_data_resp_json
                          )
                      )
                      groups = []

                  if self.check_user_in_groups(groups, self.allowed_groups):
                      user_info['admin'] = self.check_user_in_groups(groups, self.admin_groups)
                  else:
                      user_info = None

              return user_info

      c.JupyterHub.authenticator_class = GroupAuth

scheduling:
  # Try to pack users tightly rather than spinning up one node per user
  userScheduler:
    enabled: true
  # Enable us to evict a placeholder rather than spin up a whole new node
  podPriority:
    enabled: true
    defaultPriority: 0
    userPlaceholderPriority: -10 # -10 is equal to scale up criteria
  # Prep some environments beforehand so users don't have to wait
  userPlaceholder:
    enabled: true
    replicas: 4 # Number of placeholders, this should eq high mem / default mem

cull:
  enabled: true
  timeout: 86400 # seconds == 1 day
  every: 300 # 5 minutes
